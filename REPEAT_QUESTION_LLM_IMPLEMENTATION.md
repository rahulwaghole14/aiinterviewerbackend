# Repeat Question & LLM Response Implementation

## âœ… Implementation Complete

All interviewee responses now go through the LLM, and all AI responses are generated by the LLM, including repeat question requests.

---

## ğŸ”§ Key Changes

### **1. Enhanced Repeat Request Detection**

**Function: `is_repeat_request(text: str)`**

Now detects a wider range of repeat-related phrases:
- "repeat", "again", "can you repeat", "please repeat", "ask again"
- "repeat the question", "ask that question again", "say it again"
- "what was the question", "can you say that again", "pardon"
- "sorry what", "what did you say", "i didn't hear", "i didn't catch"
- "could you repeat", "would you repeat", "say that again"

**Location:** `interview_app/complete_ai_bot.py` (lines 479-493)

---

### **2. LLM-Powered Repeat Response Generation**

**New Function: `generate_repeat_question_response(session, original_question: str)`**

- Uses LLM to generate a natural, professional response that repeats the question
- Includes brief acknowledgment (e.g., "Of course", "Sure", "No problem")
- Repeats the exact same question clearly
- Keeps response concise (1-2 sentences)
- Falls back to simple repeat if LLM fails

**Location:** `interview_app/complete_ai_bot.py` (lines 495-530)

**Example LLM Prompt:**
```
You are a professional technical interviewer. The candidate has asked you to repeat the question.

Original question that was asked: [question]

Generate a natural, professional response that repeats the question.
Your response should:
1. Briefly acknowledge the request (e.g., 'Of course', 'Sure', 'No problem')
2. Repeat the exact same question clearly
3. Keep it concise and professional
4. Do NOT add any new information or change the question
5. Do NOT ask follow-up questions - just repeat the original question
```

---

### **3. Repeat Request Handling in `upload_answer`**

**Location:** `interview_app/complete_ai_bot.py` (lines 1233-1275)

**Flow:**
1. Checks if candidate's transcript contains repeat request keywords
2. If yes, retrieves the last question asked
3. Uses LLM to generate a natural repeat response
4. Returns the LLM-generated response (not hardcoded)
5. Does NOT increment question number (same question)

**Code Logic:**
```python
# Check if candidate is requesting to repeat the question
if is_repeat_request(transcript):
    last_question = get_last_strict_question(session)
    if last_question:
        # Use LLM to generate a natural response that repeats the question
        repeat_response = generate_repeat_question_response(session, last_question)
        session.add_interviewer_message(repeat_response)
        # ... return response
    else:
        # No previous question - use LLM to generate helpful response
        help_response = gemini_generate(help_prompt, max_retries=3)
        # ... return response
```

---

### **4. LLM-Powered Acknowledgment Messages**

**Location:** `interview_app/complete_ai_bot.py` (lines 1352-1375)

**Before:**
```python
return {
    "acknowledge": True,
    "message": "I didn't catch that. Please try again.",  # Hardcoded
    ...
}
```

**After:**
```python
# Use LLM to generate a natural acknowledgment response
acknowledge_prompt = (
    "You are a professional technical interviewer. The candidate's response was unclear or not detected. "
    "Generate a brief, professional, single-line response (1 sentence) asking them to try again or repeat their answer. "
    "Keep it natural and encouraging. Do NOT use phrases like 'I didn't catch that' - be more professional."
)
acknowledge_response = gemini_generate(acknowledge_prompt, max_retries=3)
# ... return LLM-generated response
```

---

## ğŸ“‹ How It Works

### **Repeat Request Flow:**

```
1. Candidate says: "Can you repeat the question?"
   â†“
2. System detects: is_repeat_request(transcript) â†’ True
   â†“
3. Retrieves: last_question = get_last_strict_question(session)
   â†“
4. LLM generates: repeat_response = generate_repeat_question_response(session, last_question)
   â†“
5. Example LLM response: "Of course. Can you explain the difference between L1 and L2 regularization?"
   â†“
6. Returns: LLM-generated response (not hardcoded)
```

### **All Responses Through LLM:**

âœ… **Interviewee Responses:**
- All candidate transcripts are sent to `upload_answer()`
- Processed through LLM via `generate_question()` for next question
- Evaluated through LLM for follow-up decisions

âœ… **AI Responses:**
- All questions: `generate_question()` â†’ LLM
- Repeat responses: `generate_repeat_question_response()` â†’ LLM
- Acknowledgment messages: `gemini_generate()` â†’ LLM
- Candidate question answers: `generate_candidate_answer()` â†’ LLM
- Final closing: `generate_final_closing()` â†’ LLM
- Proceed prompts: `generate_proceed_prompt()` â†’ LLM

---

## ğŸ¯ Key Features

### **1. Natural Repeat Responses**
- LLM generates contextually appropriate acknowledgments
- Varies responses (not repetitive)
- Professional and natural language

### **2. Fallback Handling**
- If LLM fails, uses simple fallback: "Of course. [question]"
- If no previous question, LLM generates helpful response
- Ensures system always responds

### **3. Question Number Preservation**
- Repeat requests do NOT increment question number
- Same question remains active
- Candidate can answer after repeat

### **4. Conversation Context**
- LLM has access to full conversation history
- Can generate contextually appropriate responses
- Maintains interview flow and tone

---

## ğŸ” Testing Scenarios

### **Scenario 1: Simple Repeat Request**
```
Candidate: "Can you repeat that?"
AI (LLM): "Of course. Can you explain the difference between supervised and unsupervised learning?"
```

### **Scenario 2: Polite Repeat Request**
```
Candidate: "Sorry, could you say that again?"
AI (LLM): "No problem. What are the key differences between REST and GraphQL APIs?"
```

### **Scenario 3: Direct Repeat Request**
```
Candidate: "Repeat the question please"
AI (LLM): "Sure. How would you handle missing values in a dataset?"
```

### **Scenario 4: No Previous Question**
```
Candidate: "Can you repeat?"
AI (LLM): "I don't have a previous question to repeat. Please wait for the next question or continue with your answer."
```

---

## ğŸ“Š Summary

| Feature | Status | Implementation |
|---------|--------|----------------|
| Repeat request detection | âœ… | Enhanced `is_repeat_request()` with more keywords |
| LLM repeat response | âœ… | New `generate_repeat_question_response()` function |
| All responses through LLM | âœ… | All AI responses use LLM (no hardcoded messages) |
| Interviewee responses to LLM | âœ… | All transcripts processed through LLM |
| Fallback handling | âœ… | Simple fallback if LLM fails |
| Context awareness | âœ… | LLM has full conversation context |

---

## ğŸš€ Result

âœ… **All interviewee responses** are sent to the LLM for processing  
âœ… **All AI responses** are generated by the LLM  
âœ… **Repeat requests** are detected and handled naturally using LLM  
âœ… **No hardcoded responses** - everything goes through LLM  
âœ… **Natural, professional** responses that vary based on context  

The system now provides a fully LLM-powered interview experience with natural, context-aware responses!

