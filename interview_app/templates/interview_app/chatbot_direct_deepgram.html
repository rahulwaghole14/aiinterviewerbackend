<!DOCTYPE html>
<html><head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width,initial-scale=1'>
<title>AI Interview Bot</title>
<style>
body{font-family:'Segoe UI',Tahoma,Arial;margin:0;padding:16px;background:#f5f5f5;} 
.container{background:#fff;border-radius:10px;box-shadow:0 2px 10px rgba(0,0,0,.08);padding:20px;max-width:800px;margin:0 auto;}
.status{padding:15px;border-radius:8px;background:#e7f3ff;color:#0c5460;margin:15px 0;text-align:center;font-weight:600;}
.question{font-size:18px;font-weight:600;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:8px;border-left:4px solid #007bff;}
.transcript{background:#f8f9fa;padding:15px;border-radius:8px;margin:15px 0;border-left:3px solid #28a745;min-height:60px;}
.progress{width:100%;height:20px;background:#e9ecef;border-radius:10px;overflow:hidden;margin:15px 0;}
.progress-fill{height:100%;background:#007bff;transition:width 0.3s ease;}
audio{width:100%;margin:10px 0;}

/* Audio Visualizer Styles */
.audio-visualizer-container {
  width: 100%;
  max-width: 100%;
  height: 120px;
  margin: 15px 0;
  background: #f8f9fa;
  border: 1px solid #dee2e6;
  border-radius: 12px;
  padding: 1rem;
  display: flex;
  align-items: center;
  justify-content: center;
  overflow: hidden;
}

.audio-visualizer-canvas {
  width: 100%;
  height: 100%;
  display: block;
}
</style>
</head>
<body>
<div class='container'>
  <h2 style='text-align:center;color:#333;margin-bottom:20px;'>üé§ AI Technical Interview</h2>
  <div class='progress'><div class='progress-fill' id='progressFill' style='width:0%'></div></div>
  <div style='text-align:center;margin-bottom:20px;'><span id='progressText'>Question 1 of 4</span></div>
  <div class='status' id='st'>Initializing interview...</div>
  <div class='question' id='q'></div>
  <audio id='qa' controls style='display:none'></audio>
  
  <!-- Audio Visualizer - Only shown during recording -->
  <div id='audio-visualizer-container' class='audio-visualizer-container' style='display:none;'>
    <canvas id='audio-visualizer' class='audio-visualizer-canvas'></canvas>
  </div>
  
  <div class='transcript' id='live'><strong>Live transcription:</strong> <span style='color:#999;'>Listening for speech...</span></div>
</div>

<script>
const SESSION_KEY="{{ session_key }}";
const DEEPGRAM_API_KEY='6690abf90d1c62c6b70ed632900b2c093bc06d79';

let sid=null,listening=false,isFinalizing=false;
let ws=null,audioContext=null,processor=null,source=null,streamRef=null;
let transcripts=[],currentInterim='';
let recordingStartMs=0,lastSpeechMs=0;
const INITIAL_SILENCE_MS=5000,SPEECH_INACTIVITY_MS=5000;

// Audio Visualizer variables
let audioAnalyser=null;
let visualizerCanvas=null;
let visualizerCtx=null;
let visualizerDataArray=null;
let visualizerAnimationId=null;

async function api(url,body){
  const r=await fetch(url,{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify(body)});
  return await r.json();
}

async function start(){
  console.log('üöÄ Starting chatbot...');
  document.getElementById('st').innerText='Starting...';
  const data=await api('/ai/start',{session_key:SESSION_KEY});
  console.log('‚úÖ Response from /ai/start:',data);
  if(data.error){
    document.getElementById('st').innerText='Error: '+data.error;
    return;
  }
  sid=data.session_id;
  const questionText=data.question||data.next_question||'';
  console.log('üìù Question text:',questionText);
  
  // Always show question text if available
  if(questionText){
    document.getElementById('q').innerText=questionText;
  }else{
    console.log('‚ö†Ô∏è No question text provided, will show when audio plays');
  }
  
  document.getElementById('progressText').innerText='Question '+data.question_number+' of '+data.max_questions;
  document.getElementById('progressFill').style.width=((data.question_number/data.max_questions)*100)+'%';
  
  if(data.audio_url && data.audio_url.trim()){
    document.getElementById('st').innerText='üîä Playing question audio...';
    const qa=document.getElementById('qa');
    qa.style.display='block';
    qa.src=data.audio_url;
    qa.onended=()=>{
      document.getElementById('st').innerText='üé§ Speak your answer now...';
      beginRecord();
    };
    qa.play().catch(()=>{
      document.getElementById('st').innerText='üé§ Speak your answer now...';
      beginRecord();
    });
  }else{
    // Use browser's built-in TTS as fallback when Google Cloud TTS is not available
    if(questionText && 'speechSynthesis' in window){
      document.getElementById('st').innerText='üîä Speaking question...';
      const utterance=new SpeechSynthesisUtterance(questionText);
      utterance.rate=0.9;
      utterance.pitch=1.0;
      utterance.volume=1.0;
      utterance.onend=()=>{
        document.getElementById('st').innerText='üé§ Speak your answer now...';
        setTimeout(beginRecord,600);
      };
      utterance.onerror=()=>{
        document.getElementById('st').innerText='üé§ Speak your answer now...';
        setTimeout(beginRecord,600);
      };
      speechSynthesis.speak(utterance);
    }else{
      document.getElementById('st').innerText='üé§ Speak your answer now...';
      setTimeout(beginRecord,600);
    }
  }
}

async function beginRecord(){
  if(listening)return;
  listening=true;
  transcripts=[];
  document.getElementById('st').innerText='üé§ Listening...';
  document.getElementById('live').innerHTML='<strong>Live transcription:</strong> <span style="color:#999">Listening...</span>';
  
  // Get microphone if not already available
  if(!streamRef || !streamRef.active){
    try{
      console.log('üé§ Requesting microphone access...');
      streamRef=await navigator.mediaDevices.getUserMedia({
        audio:{
          echoCancellation:true,  // Enable to reduce background noise
          noiseSuppression:true,  // Enable to reduce ambient noise
          autoGainControl:true,   // Enable to boost quiet voices
          sampleRate:16000        // Optimal for speech recognition
        }
      });
      console.log('‚úÖ Microphone access granted');
      const track=streamRef.getAudioTracks()[0];
      console.log('üé§ Track:', track.label, 'Settings:', track.getSettings());
    }catch(e){
      console.error('‚ùå Microphone error:', e);
      document.getElementById('st').innerText='Mic error: '+e.message;
      return;
    }
  }else{
    console.log('‚ôªÔ∏è Reusing existing microphone stream');
  }
  
  // Create audio context if needed
  if(!audioContext || audioContext.state==='closed'){
    console.log('üéôÔ∏è Creating audio context...');
    audioContext=new (window.AudioContext||window.webkitAudioContext)();
    console.log('üéôÔ∏è Audio context SR:', audioContext.sampleRate);
  }else{
    console.log('‚ôªÔ∏è Reusing audio context, state:', audioContext.state);
  }
  
  // Resume if suspended
  if(audioContext.state==='suspended'){
    await audioContext.resume();
    console.log('‚úÖ Audio context resumed');
  }
  
  // Create source if needed (ONLY ONCE!)
  if(!source){
    source=audioContext.createMediaStreamSource(streamRef);
    console.log('‚úÖ Media stream source created');
  }else{
    console.log('‚ôªÔ∏è Reusing media stream source');
  }
  
  // Create analyser node for audio visualizer
  if(!audioAnalyser){
    audioAnalyser=audioContext.createAnalyser();
    audioAnalyser.fftSize=256; // Number of frequency bins
    audioAnalyser.smoothingTimeConstant=0.8; // Smoothing factor
    source.connect(audioAnalyser);
    console.log('‚úÖ Audio analyser created for visualizer');
  }
  
  // Connect to Deepgram DIRECTLY - USING SAME SETTINGS AS portal.html
  const sampleRate=audioContext.sampleRate;
  // ‚úÖ OPTIMIZED SETTINGS FOR CONTINUOUS SPEECH (matching portal.html)
  const deepgramUrl=`wss://api.deepgram.com/v1/listen?` + new URLSearchParams({
    token: DEEPGRAM_API_KEY,
    model: 'nova-2',              // Better accuracy than 'general' or 'nova-3'
    tier: 'enhanced',             // Enhanced tier for better accuracy
    encoding: 'linear16',
    sample_rate: sampleRate,
    channels: '1',
    
    // üîë KEY SETTINGS
    interim_results: 'true',      // Real-time partial transcripts
    filler_words: 'true',         // Capture "um", "uh", etc.
    speech_final: 'true',         // Send final speech results
    
    // üéØ CRITICAL: Prevent premature finalization during continuous speech
    endpointing: '10000',          // 10 seconds - allows long continuous speech without premature finalization
    utterance_end_ms: '10000',    // 10 seconds - matches endpointing for consistency
    utterance_silence_threshold_ms: '5000',  // 5 seconds - silence threshold for utterance boundaries
    vad_turnoff: '5000',          // 5 seconds VAD delay - prevents aggressive cutoffs during pauses
    vad_threshold: '0.5',         // VAD sensitivity threshold (0.0-1.0) - 0.5 is balanced
    vad_sensitivity: '0.2',       // VAD sensitivity (0.0-1.0) - 0.2 is more sensitive to detect speech
    
    // üìä Output formatting
    smart_format: 'false',
    punctuate: 'false',
    diarize: 'false',
    
    // ‚ö° Performance
    no_delay: 'true'              // Real-time streaming
  }).toString();
  
  // Validate API key before attempting connection
  if(!DEEPGRAM_API_KEY || DEEPGRAM_API_KEY.trim() === ''){
    console.error('‚ùå Deepgram API key is missing or empty');
    document.getElementById('st').innerText='‚ùå Error: Deepgram API key is missing. Please configure the API key.';
    listening=false;
    return;
  }
  
  // Validate API key format (Deepgram API keys are typically 40+ characters)
  if(DEEPGRAM_API_KEY.length < 20){
    console.warn('‚ö†Ô∏è Deepgram API key seems too short. Expected 40+ characters.');
  }
  
  console.log('üîó Connecting directly to Deepgram (nova-2, enhanced tier, optimized for continuous speech)...');
  console.log('üîó Sample Rate:', sampleRate);
  console.log('üîó API Key length:', DEEPGRAM_API_KEY.length);
  console.log('üîó API Key format valid:', /^[a-zA-Z0-9]+$/.test(DEEPGRAM_API_KEY));
  
  // Declare ws variable first
  let ws;
  let connectionTimeout;
  
  try{
    // Create WebSocket connection
    console.log('üîó Creating WebSocket connection...');
    ws=new WebSocket(deepgramUrl);
    ws.binaryType='arraybuffer';
    console.log('üîó WebSocket object created, initial state:', ws.readyState);
    console.log('üîó WebSocket URL length:', deepgramUrl.length);
    
    // Set connection timeout (10 seconds) after ws is created
    connectionTimeout = setTimeout(() => {
      if(ws && ws.readyState === WebSocket.CONNECTING){
        console.error('‚ùå WebSocket connection timeout after 10 seconds');
        console.error('   Final state:', ws.readyState);
        ws.close();
        document.getElementById('st').innerText='‚ùå Connection timeout. Please check your internet connection and Deepgram API key.';
        listening=false;
      }
    }, 10000);
  }catch(error){
    console.error('‚ùå Failed to create WebSocket:', error);
    console.error('   Error name:', error.name);
    console.error('   Error message:', error.message);
    document.getElementById('st').innerText='Error: Failed to create WebSocket connection. ' + error.message;
    listening=false;
    return;
  }
  
  ws.onopen=(ev)=>{
    console.log('‚úÖ Connected to Deepgram!');
    console.log('   Protocol:', ws.protocol);
    console.log('   Extensions:', ws.extensions);
    clearTimeout(connectionTimeout); // Clear timeout on successful connection
    document.getElementById('st').innerText='üé§ Listening...';
    
    // Create gain node to amplify audio
    const gainNode=audioContext.createGain();
    gainNode.gain.value=3.0;  // Amplify by 3x for better detection
    console.log('üîä Gain node created: 3x amplification');
    
    // Create ScriptProcessor - MATCH ORIGINAL: MONO input (1 channel)
    console.log('üîß Creating ScriptProcessor...');
    processor=audioContext.createScriptProcessor(4096,1,1);  // Changed from (4096,2,1) to (4096,1,1)
    let packetCount=0;
    let soundDetectedCount=0;
    
    processor.onaudioprocess=(e)=>{
      if(ws.readyState!==WebSocket.OPEN)return;
      
      packetCount++;
      
      // Get MONO audio data (channel 0 only)
      const audioData=e.inputBuffer.getChannelData(0);
      
      // Debug: Check if audio data is all zeros
      if(packetCount === 1){
        let allZeros = true;
        for(let i=0; i<Math.min(100, audioData.length); i++){
          if(audioData[i] !== 0){
            allZeros = false;
            break;
          }
        }
        console.log('üîç First packet audio check - All zeros?', allZeros);
        console.log('üîç Sample values - [0]:', audioData[0], '[1]:', audioData[1], '[2]:', audioData[2]);
        console.log('üîç Buffer length:', audioData.length, 'Channels:', e.inputBuffer.numberOfChannels);
      }
      
      // Calculate RMS (same as original app.py)
      let sum=0;
      for(let i=0;i<audioData.length;i++){
        sum+=audioData[i]*audioData[i];
      }
      const rms=Math.sqrt(sum/audioData.length);
      
      if(packetCount % 50 === 1){
        console.log('üéµ Packet#', packetCount, '| RMS:', rms.toFixed(6), '| Sound detected:', soundDetectedCount, 'times');
      }
      
      if(rms>0.001){  // Lower threshold to detect quieter audio
        soundDetectedCount++;
        if(soundDetectedCount % 10 === 1){
          console.log('üîä Sound detected! RMS:', rms.toFixed(4), '| Total detections:', soundDetectedCount);
        }
      }
      
      // Convert to INT16 (same as original)
      const int16=new Int16Array(audioData.length);
      for(let i=0;i<audioData.length;i++){
        let s=audioData[i];
        s=Math.max(-1,Math.min(1,s));
        int16[i]=s<0?s*0x8000:s*0x7FFF;
      }
      
      ws.send(int16.buffer);
    };
    
    // Connect audio graph
    source.connect(processor);
    processor.connect(audioContext.destination);
    console.log('‚úÖ Audio graph connected');
    
    // Keep references
    window._processor=processor;
    window._source=source;
    window._audioContext=audioContext;
  };
  
  let messageCount = 0;
  ws.onmessage=(ev)=>{
    try{
      messageCount++;
      const data=JSON.parse(ev.data);
      if(messageCount % 10 === 1){
        console.log('üì® Deepgram message#', messageCount, ':', data);
      }
      if(data.channel && data.channel.alternatives && data.channel.alternatives[0]){
        const t=data.channel.alternatives[0].transcript||'';
        const isFinal=!!data.is_final;
        if(isFinal && t.trim()){
          console.log('üìù FINAL:', t);
          transcripts.push(t);
          currentInterim='';  // Clear interim
          lastSpeechMs=Date.now();  // Update speech timestamp
          updateLive();
        }else if(t.trim()){
          console.log('üìù interim:', t);
          currentInterim=t;  // Store interim
          lastSpeechMs=Date.now();  // Update speech timestamp
          updateLive();  // Update display with interim
        }else{
          if(messageCount % 10 === 1){
            console.log('üìù Empty transcript');
          }
        }
      }
    }catch(err){
      console.error('‚ùå Parse error:', err, ev.data);
    }
  };
  
  ws.onclose=(ev)=>{
    clearTimeout(connectionTimeout); // Clear timeout on close
    console.log('üîå Deepgram WebSocket closed');
    console.log('   Code:', ev.code);
    console.log('   Reason:', ev.reason || 'No reason provided');
    console.log('   Was Clean:', ev.wasClean);
    
    // Show user-friendly error message
    if(ev.code !== 1000 && listening){ // 1000 = normal closure
      let errorMsg = 'Connection closed';
      if(ev.code === 1006){
        errorMsg = 'Connection failed. Please check your internet connection or Deepgram API key.';
      }else if(ev.code === 1002){
        errorMsg = 'Protocol error. Please refresh the page.';
      }else if(ev.code === 1003){
        errorMsg = 'Invalid data. Please try again.';
      }else if(ev.code === 1008){
        errorMsg = 'Policy violation. Please check Deepgram API key validity.';
      }else if(ev.code === 1011){
        errorMsg = 'Server error. Please try again later.';
      }else if(ev.code === 4000){
        errorMsg = 'Authentication failed. Please check Deepgram API key.';
      }
      document.getElementById('st').innerText = `‚ùå ${errorMsg} (Code: ${ev.code})`;
    }
    
    // Reset listening state if not clean close
    if(!ev.wasClean){
      listening=false;
    }
  };
  
  ws.onerror=(ev)=>{
    clearTimeout(connectionTimeout); // Clear timeout on error
    console.error('‚ùå Deepgram WebSocket error:', ev);
    console.error('   Error type:', ev.type);
    console.error('   Target readyState:', ev.target?.readyState);
    console.error('   Target URL (first 100 chars):', ev.target?.url?.substring(0, 100) || 'No URL');
    console.error('   Error details:', ev.message || 'No message provided');
    
    // Check readyState to determine error type
    const readyState = ev.target?.readyState;
    let errorMsg = 'Connection error. ';
    
    if(readyState === WebSocket.CONNECTING){
      errorMsg += 'Failed to connect. ';
      errorMsg += 'Please verify your Deepgram API key is valid and has proper permissions.';
    }else if(readyState === WebSocket.CLOSING || readyState === WebSocket.CLOSED){
      errorMsg += 'Connection closed unexpectedly. ';
      errorMsg += 'This usually indicates an authentication failure. ';
      errorMsg += 'Please check that your Deepgram API key is valid and not expired.';
    }else{
      errorMsg += 'Please check your internet connection and Deepgram API key.';
    }
    
    document.getElementById('st').innerText = `‚ùå ${errorMsg}`;
    listening=false;
    
    // Additional debugging
    console.error('   Full error context:', {
      type: ev.type,
      readyState: readyState,
      readyStateName: readyState === 0 ? 'CONNECTING' : readyState === 1 ? 'OPEN' : readyState === 2 ? 'CLOSING' : readyState === 3 ? 'CLOSED' : 'UNKNOWN',
      hasTarget: !!ev.target,
      urlLength: ev.target?.url?.length || 0
    });
  };
  
  // REMOVED: Auto-finalize functionality - no longer automatically processes answers
  // Users must explicitly click to finish/submit their answer
  console.log('‚úÖ Recording started - waiting for explicit submission...');
  recordingStartMs=Date.now();
  lastSpeechMs=recordingStartMs;
  // tickInactive(); // DISABLED: No auto-finalize - user must submit manually
  
  // Start audio visualizer
  startAudioVisualizer();
}

function updateLive(){
  const final=transcripts.join(' ').trim();
  const combined=(final+(currentInterim?' '+currentInterim:'')).trim();
  
  let html='<strong>Live transcription:</strong> ';
  if(final){
    html+=`<span style="color:#333">${final}</span>`;
  }
  if(currentInterim){
    html+=` <span style="color:#007bff; background:#e7f3ff; padding:2px 4px; border-radius:3px">${currentInterim}</span>`;
  }
  if(!final && !currentInterim){
    html+='<span style="color:#999">Listening...</span>';
  }
  
  document.getElementById('live').innerHTML=html;
}

// DISABLED: Auto-finalize functionality - removed to prevent processing answers within 2-3 seconds
// Users must explicitly submit their answers
function tickInactive(){
  // REMOVED: All auto-finalize logic
  // This function is kept for compatibility but does nothing
  // Users must manually click to finish/submit their answer
  return;
  
  // OLD CODE (DISABLED):
  // if(!listening)return;
  // const now=Date.now();
  // const noSpeechDuration=now-lastSpeechMs;
  // const initialSilenceDuration=now-recordingStartMs;
  // 
  // // Auto-finalize if: 5s initial silence OR 5s after last speech
  // if(transcripts.length===0 && initialSilenceDuration>=INITIAL_SILENCE_MS){
  //   console.log('‚è∞ Initial silence timeout (5s) - finalizing...');
  //   finalize();
  //   return;
  // }
  // 
  // if(transcripts.length>0 && noSpeechDuration>=SPEECH_INACTIVITY_MS){
  //   console.log('‚è∞ Speech inactivity timeout (5s) - finalizing...');
  //   console.log(`   Last speech: ${Math.floor(noSpeechDuration/1000)}s ago`);
  //   finalize();
  //   return;
  // }
  // 
  // // Check again in 500ms
  // setTimeout(tickInactive,500);
}

// Audio Visualizer Functions
function startAudioVisualizer(){
  if(!audioAnalyser || !audioContext){
    console.log('‚ö†Ô∏è Audio analyser not available for visualizer');
    return;
  }
  
  // Get canvas element
  visualizerCanvas=document.getElementById('audio-visualizer');
  if(!visualizerCanvas){
    console.log('‚ö†Ô∏è Audio visualizer canvas not found');
    return;
  }
  
  visualizerCtx=visualizerCanvas.getContext('2d');
  visualizerCanvas.width=visualizerCanvas.offsetWidth;
  visualizerCanvas.height=visualizerCanvas.offsetHeight;
  
  // Create data array for frequency analysis
  const bufferLength=audioAnalyser.frequencyBinCount;
  visualizerDataArray=new Uint8Array(bufferLength);
  
  // Show visualizer container
  const visualizerContainer=document.getElementById('audio-visualizer-container');
  if(visualizerContainer){
    visualizerContainer.style.display='block';
  }
  
  // Start animation loop
  drawAudioVisualizer();
}

function drawAudioVisualizer(){
  if(!visualizerCanvas || !visualizerCtx || !audioAnalyser || !visualizerDataArray){
    return;
  }
  
  // Only continue if visualizer is visible and listening
  if(!listening){
    stopAudioVisualizer();
    return;
  }
  
  // Get frequency data
  audioAnalyser.getByteFrequencyData(visualizerDataArray);
  
  // Clear canvas
  visualizerCtx.fillStyle='#f8f9fa';
  visualizerCtx.fillRect(0,0,visualizerCanvas.width,visualizerCanvas.height);
  
  // Draw bars
  const barCount=60; // Number of bars to display
  const barWidth=visualizerCanvas.width/barCount;
  const barSpacing=2;
  const maxBarHeight=visualizerCanvas.height-20;
  
  for(let i=0;i<barCount;i++){
    // Map frequency data to bars (use multiple bins per bar for smoother visualization)
    const dataIndex=Math.floor((i/barCount)*visualizerDataArray.length);
    const barHeight=(visualizerDataArray[dataIndex]/255)*maxBarHeight;
    
    // Create gradient for bars
    const gradient=visualizerCtx.createLinearGradient(0,visualizerCanvas.height,0,visualizerCanvas.height-barHeight);
    gradient.addColorStop(0,'#4CAF50'); // Green at bottom
    gradient.addColorStop(0.5,'#FFC107'); // Yellow in middle
    gradient.addColorStop(1,'#FF5722'); // Red at top
    
    // Draw bar
    const x=i*(barWidth+barSpacing);
    visualizerCtx.fillStyle=gradient;
    visualizerCtx.fillRect(x,visualizerCanvas.height-barHeight,barWidth-barSpacing,barHeight);
  }
  
  // Continue animation
  visualizerAnimationId=requestAnimationFrame(drawAudioVisualizer);
}

function stopAudioVisualizer(){
  // Stop animation
  if(visualizerAnimationId){
    cancelAnimationFrame(visualizerAnimationId);
    visualizerAnimationId=null;
  }
  
  // Hide visualizer container
  const visualizerContainer=document.getElementById('audio-visualizer-container');
  if(visualizerContainer){
    visualizerContainer.style.display='none';
  }
  
  // Clear canvas
  if(visualizerCanvas && visualizerCtx){
    visualizerCtx.clearRect(0,0,visualizerCanvas.width,visualizerCanvas.height);
  }
  
  // Clean up variables
  visualizerCanvas=null;
  visualizerCtx=null;
  visualizerDataArray=null;
}

async function finalize(){
  if(isFinalizing)return;
  isFinalizing=true;
  listening=false;
  console.log('üé¨ Finalizing...');
  document.getElementById('st').innerText='Processing your answer...';
  
  // Stop audio visualizer
  stopAudioVisualizer();
  
  // Stop processor
  if(processor){
    processor.disconnect();
    processor=null;
    console.log('‚èπÔ∏è Processor stopped');
  }
  
  // Close WebSocket
  if(ws){
    ws.close();
    ws=null;
  }
  
  const transcript=transcripts.join(' ').trim();
  console.log('üìù Final transcript:', transcript);
  
  const resp=await api('/ai/upload_answer',{
    session_id:sid,
    transcript:transcript||'[No speech detected]',
    question_number:1  // Will be tracked by backend
  });
  
  console.log('üì• Response:', resp);
  
  // Handle acknowledge (no speech detected) - just restart recording
  if(resp.acknowledge){
    console.log('‚ö†Ô∏è Acknowledge response - no speech detected, retrying...');
    isFinalizing=false;
    transcripts=[];
    currentInterim='';
    updateLive();
    document.getElementById('st').innerText='‚ùå No speech detected. Please speak clearly...';
    setTimeout(()=>{
      document.getElementById('st').innerText='üé§ Speak your answer now...';
      beginRecord();
    },1500);
    return;
  }
  
  if(resp.completed){
    isFinalizing=false;
    document.getElementById('st').innerText='üéâ Technical Q&A Complete!';
    
    // Check if there are coding questions
    const hasCodingRound = resp.has_coding_questions !== false;  // Default to true
    
    if(hasCodingRound){
      document.getElementById('q').innerHTML=`
        <div style="text-align:center; padding:2rem;">
          <h3 style="color:#28a745; margin-bottom:1rem;">Great job completing the technical questions!</h3>
          <p style="color:#666; margin-bottom:2rem;">Now let's move to the coding challenge.</p>
          <button onclick="startCodingRound()" style="background:#007bff; color:white; padding:12px 24px; border:none; border-radius:8px; font-size:16px; cursor:pointer; box-shadow:0 2px 8px rgba(0,123,255,0.3);">
            üíª Start Coding Challenge
          </button>
        </div>
      `;
    }else{
      // No coding round - show completion with PDF link
      const pdfUrl = '/ai/transcript_pdf?session_key='+SESSION_KEY;
      document.getElementById('q').innerHTML=`
        <div style="text-align:center; padding:2rem;">
          <h3 style="color:#28a745; margin-bottom:1rem;">üéâ Interview Complete!</h3>
          <p style="color:#666; margin-bottom:1.5rem;">Thank you for completing the interview.</p>
          <a href="${pdfUrl}" target="_blank" style="background:#28a745; color:white; padding:12px 24px; border:none; border-radius:8px; font-size:16px; text-decoration:none; display:inline-block; box-shadow:0 2px 8px rgba(40,167,69,0.3);">
            üìÑ Download Interview Transcript
          </a>
        </div>
      `;
    }
    document.getElementById('live').style.display='none';
  }else{
    // Reset for next question
    transcripts=[];
    currentInterim='';
    updateLive();
    isFinalizing=false;
    
    const nextQuestionText=resp.question||resp.next_question||'';
    console.log('üìù Next question text:',nextQuestionText);
    console.log('üì• Full response:',resp);
    
    // Update question text ONLY if we have text
    if(nextQuestionText){
      document.getElementById('q').innerText=nextQuestionText;
    }else{
      console.log('‚ö†Ô∏è No question text, keeping previous question visible');
    }
    
    document.getElementById('progressText').innerText='Question '+resp.question_number+' of '+(resp.max_questions || 4);
    document.getElementById('progressFill').style.width=((resp.question_number/(resp.max_questions || 4))*100)+'%';
    
    if(resp.audio_url){
      document.getElementById('st').innerText='üîä Playing next question...';
      const qa=document.getElementById('qa');
      qa.style.display='block';
      qa.src=resp.audio_url;
      qa.onended=()=>{
        document.getElementById('st').innerText='üé§ Speak your answer now...';
        beginRecord();
      };
      qa.play().catch(e=>{
        console.log('‚ö†Ô∏è Audio play failed, starting recording anyway:', e);
        document.getElementById('st').innerText='üé§ Speak your answer now...';
        beginRecord();
      });
    }else{
      // Use browser's built-in TTS as fallback when Google Cloud TTS is not available
      if(nextQuestionText && 'speechSynthesis' in window){
        console.log('‚ö†Ô∏è No audio URL, using browser TTS fallback');
        document.getElementById('st').innerText='üîä Speaking question...';
        const utterance=new SpeechSynthesisUtterance(nextQuestionText);
        utterance.rate=0.9;
        utterance.pitch=1.0;
        utterance.volume=1.0;
        utterance.onend=()=>{
          document.getElementById('st').innerText='üé§ Speak your answer now...';
          setTimeout(beginRecord,600);
        };
        utterance.onerror=()=>{
          document.getElementById('st').innerText='üé§ Speak your answer now...';
          setTimeout(beginRecord,600);
        };
        speechSynthesis.speak(utterance);
      }else{
        console.log('‚ö†Ô∏è No audio URL in response, starting recording immediately');
        document.getElementById('st').innerText='üé§ Speak your answer now...';
        setTimeout(beginRecord,600);
      }
    }
  }
}

// Function to start coding round (called from button in iframe)
function startCodingRound(){
  console.log('üíª Starting coding round...');
  // Notify parent window to start coding phase
  window.parent.postMessage({type:'start_coding_round'},'*');
}

// Auto-start on load
setTimeout(()=>{
  console.log('üöÄ Auto-starting interview...');
  start();
},500);
</script>
</body>
</html>

