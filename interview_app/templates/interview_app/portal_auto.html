<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview Portal</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 20px;
        }
        .main-content {
            background: white;
            border-radius: 12px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .proctoring-sidebar {
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .status {
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            text-align: center;
            font-size: 16px;
            font-weight: 600;
        }
        .status.info {
            background-color: #e7f3ff;
            color: #0c5460;
        }
        .status.success {
            background-color: #d4edda;
            color: #155724;
        }
        .status.error {
            background-color: #f8d7da;
            color: #721c24;
        }
        .question {
            font-size: 18px;
            font-weight: 600;
            margin: 20px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 3px solid #28a745;
            min-height: 120px;
        }
        .progress {
            width: 100%;
            height: 20px;
            background: #e9ecef;
            border-radius: 10px;
            overflow: hidden;
            margin: 20px 0;
        }
        .progress-fill {
            height: 100%;
            background: #007bff;
            transition: width 0.3s ease;
        }
        .mic-status {
            display: inline-block;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            margin-right: 10px;
            background: #dc3545;
            animation: pulse 1s infinite;
        }
        .mic-status.idle {
            background: #6c757d;
            animation: none;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }
        audio {
            width: 100%;
            margin: 15px 0;
        }
        .hidden {
            display: none;
        }
        .btn {
            background: #28a745;
            color: #fff;
            border: none;
            border-radius: 8px;
            padding: 12px 24px;
            cursor: pointer;
            font-size: 16px;
            margin: 10px;
        }
        .btn:hover {
            background: #218838;
        }
        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }
        .video-container {
            width: 100%;
            height: 200px;
            background: #000;
            border-radius: 8px;
            margin: 10px 0;
        }
        .warning {
            background: #fff3cd;
            color: #856404;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="main-content">
            <h1 style="text-align: center; color: #333; margin-bottom: 30px;">üé§ AI Technical Interview</h1>
            
            <div class="progress">
                <div class="progress-fill" id="progressFill" style="width: 0%"></div>
            </div>
            <div style="text-align: center; margin-bottom: 20px;">
                <span id="progressText">Question 1 of 8</span>
            </div>
            
            <div class="status info" id="status">Starting technical interview...</div>
            <div class="question" id="question"></div>
            <audio id="questionAudio" controls style="display: none;"></audio>
            
            <div style="text-align: center; margin: 20px 0;">
                <div class="mic-status" id="micStatus"></div>
                <span id="micStatusText">Preparing microphone...</span>
            </div>
            
            <div class="transcript" id="liveTranscript">
                <strong>Live transcription:</strong> <span style="color: #999;">Listening for speech...</span>
            </div>
            
            <div style="text-align: center; margin-top: 20px;">
                <button id="finishBtn" class="btn hidden">Finish Q&A</button>
            </div>
        </div>
        
        <div class="proctoring-sidebar">
            <h3 style="margin-top: 0;">üîç Proctoring Monitor</h3>
            <div class="video-container" id="videoContainer">
                <video id="proctoringVideo" autoplay muted style="width: 100%; height: 100%; object-fit: cover;"></video>
            </div>
            <div id="sessionInfo" style="margin: 10px 0; font-size: 14px; color: #666;"></div>
            <div id="warnings" class="warning hidden">
                <strong>‚ö†Ô∏è Active Warnings</strong>
                <div id="warningList"></div>
            </div>
        </div>
    </div>

    <script>
        // Core variables
        let sessionId = null;
        let isRecording = false;
        let isFinalizing = false;
        let currentQuestionNumber = 1;
        let maxQuestions = 8;
        
        // Deepgram WebSocket variables
        let deepgramWebSocket = null;
        let audioContext = null;
        let realtimeProcessor = null;
        
        // Transcription variables
        let stableRealtimeText = '';
        let partialRealtimeText = '';
        let hadTranscript = false;
        let recordingStartMs = 0;
        let lastTranscriptMs = 0;
        
        // Timing constants
        const INITIAL_SILENCE_MS = 4000;
        const SPEECH_INACTIVITY_MS = 5000;
        const DEEPGRAM_API_KEY = '6690abf90d1c62c6b70ed632900b2c093bc06d79';
        
        // DOM elements
        const statusEl = document.getElementById('status');
        const questionEl = document.getElementById('question');
        const liveTranscriptEl = document.getElementById('liveTranscript');
        const finishBtn = document.getElementById('finishBtn');
        const questionAudio = document.getElementById('questionAudio');
        const micStatus = document.getElementById('micStatus');
        const micStatusText = document.getElementById('micStatusText');
        const progressFill = document.getElementById('progressFill');
        const progressText = document.getElementById('progressText');
        
        // Deepgram WebSocket URL - use Django proxy
        function getDeepgramWsUrl(sampleRate) {
            const loc = window.location;
            const wsProto = loc.protocol === 'https:' ? 'wss' : 'ws';
            return `${wsProto}://${loc.host}/dg_ws`;
        }
        
        // API helper
        async function api(url, body) {
            const r = await fetch(url, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(body)
            });
            return await r.json();
        }
        
        // Start the interview automatically
        async function startInterview() {
            try {
                statusEl.innerText = 'Starting technical interview...';
                updateMicStatus('preparing');
                
                // Get session key from URL
                const urlParams = new URLSearchParams(window.location.search);
                const sessionKey = urlParams.get('session_key');
                
                if (!sessionKey) {
                    statusEl.innerText = 'Error: No session key found in URL';
                    return;
                }
                
                // Get data from database using session_key
                const data = await api('/ai/start', {
                    session_key: sessionKey
                });
                
                if (data.error) {
                    statusEl.innerText = 'Error: ' + data.error;
                    return;
                }
                
                sessionId = data.session_id;
                currentQuestionNumber = data.question_number || 1;
                maxQuestions = data.max_questions || 8;
                
                // Update progress
                updateProgress(currentQuestionNumber, maxQuestions);
                
                // Display first question
                questionEl.innerText = data.question || '';
                statusEl.innerText = 'Playing question...';
                
                // Show finish button
                finishBtn.classList.remove('hidden');
                
                // Play question audio and start recording
                if (data.audio_url) {
                    questionAudio.style.display = 'block';
                    questionAudio.src = data.audio_url;
                    questionAudio.onended = () => {
                        statusEl.innerText = 'Listening for your answer...';
                        beginRecording();
                    };
                    questionAudio.play().catch(() => {
                        beginRecording();
                    });
                } else {
                    setTimeout(beginRecording, 600);
                }
                
            } catch (error) {
                statusEl.innerText = 'Error starting interview: ' + error.message;
                console.error('Start interview error:', error);
            }
        }
        
        // Start recording with Deepgram WebSocket
        async function beginRecording() {
            if (isRecording || isFinalizing) return;
            
            try {
                isRecording = true;
                updateMicStatus('recording');
                
                // Get microphone access
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: false,
                        sampleRate: 48000,
                        channelCount: 1
                    }
                });
                
                // Setup audio context
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 48000
                });
                
                const source = audioContext.createMediaStreamSource(stream);
                
                // Setup Deepgram WebSocket via Django proxy
                const sr = audioContext.sampleRate;
                const url = getDeepgramWsUrl(sr);
                deepgramWebSocket = new WebSocket(url);
                deepgramWebSocket.binaryType = 'arraybuffer';
                
                deepgramWebSocket.onopen = () => {
                    console.log('WebSocket connected to Django proxy');
                    // Send configuration first
                    const config = {
                        sample_rate: sr,
                        model: 'nova-2-meeting',
                        language: 'en'
                    };
                    deepgramWebSocket.send(JSON.stringify(config));
                    updateMicStatus('recording');
                };
                
                deepgramWebSocket.onmessage = (event) => {
                    if (typeof event.data === 'string') {
                        const data = JSON.parse(event.data);
                        handleRealtimeTranscriptionResult(data);
                    } else {
                        // Handle binary data (shouldn't happen with proxy)
                        console.log('Received binary data from proxy');
                    }
                };
                
                deepgramWebSocket.onerror = (error) => {
                    console.error('WebSocket proxy error:', error);
                    statusEl.innerText = 'Transcription error. Please try again.';
                };
                
                deepgramWebSocket.onclose = () => {
                    console.log('WebSocket proxy closed');
                };
                
                // Setup audio processing
                setupRealtimeAudioProcessing(source);
                
                // Reset transcription variables
                stableRealtimeText = '';
                partialRealtimeText = '';
                hadTranscript = false;
                recordingStartMs = Date.now();
                lastTranscriptMs = recordingStartMs;
                
                // Start inactivity monitoring
                tickInactive();
                
            } catch (error) {
                console.error('Recording error:', error);
                statusEl.innerText = 'Microphone error: ' + error.message;
                isRecording = false;
                updateMicStatus('idle');
            }
        }
        
        // Setup real-time audio processing
        function setupRealtimeAudioProcessing(source) {
            try {
                // Prefer AudioWorkletNode (modern) and fallback to ScriptProcessorNode (legacy)
                const setupWithWorklet = async () => {
                    const workletCode = `
                        class PCMWorkletProcessor extends AudioWorkletProcessor {
                            process(inputs) {
                                const input = inputs && inputs[0] && inputs[0][0];
                                if (input) {
                                    this.port.postMessage(input);
                                }
                                return true;
                            }
                        }
                        registerProcessor('pcm-worklet', PCMWorkletProcessor);
                    `;
                    const blob = new Blob([workletCode], { type: 'application/javascript' });
                    const url = URL.createObjectURL(blob);
                    await audioContext.audioWorklet.addModule(url);
                    URL.revokeObjectURL(url);
                    
                    const node = new AudioWorkletNode(audioContext, 'pcm-worklet', {
                        numberOfInputs: 1,
                        numberOfOutputs: 1,
                        outputChannelCount: [1]
                    });
                    
                    node.port.onmessage = (event) => {
                        if (!deepgramWebSocket || deepgramWebSocket.readyState !== WebSocket.OPEN) return;
                        const audioData = event.data;
                        const int16Data = new Int16Array(audioData.length);
                        for (let i = 0; i < audioData.length; i++) {
                            let s = audioData[i];
                            s = Math.max(-1, Math.min(1, s));
                            int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }
                        // Send binary data to WebSocket proxy
                        deepgramWebSocket.send(int16Data.buffer);
                    };
                    
                    const silentGain = audioContext.createGain();
                    silentGain.gain.value = 0;
                    source.connect(node);
                    node.connect(silentGain).connect(audioContext.destination);
                    realtimeProcessor = node;
                };
                
                const setupWithScriptProcessor = () => {
                    const node = audioContext.createScriptProcessor(4096, 1, 1);
                    node.onaudioprocess = (event) => {
                        if (!deepgramWebSocket || deepgramWebSocket.readyState !== WebSocket.OPEN) return;
                        const audioData = event.inputBuffer.getChannelData(0);
                        const int16Data = new Int16Array(audioData.length);
                        for (let i = 0; i < audioData.length; i++) {
                            let s = audioData[i];
                            s = Math.max(-1, Math.min(1, s));
                            int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }
                        // Send binary data to WebSocket proxy
                        deepgramWebSocket.send(int16Data.buffer);
                    };
                    source.connect(node);
                    node.connect(audioContext.destination);
                    realtimeProcessor = node;
                };
                
                if (audioContext.audioWorklet) {
                    setupWithWorklet().catch((err) => {
                        console.warn('AudioWorklet setup failed, falling back to ScriptProcessor:', err);
                        setupWithScriptProcessor();
                    });
                } else {
                    setupWithScriptProcessor();
                }
                
            } catch (error) {
                console.error('Failed to setup real-time audio processing:', error);
            }
        }
        
        // Handle real-time transcription results
        function handleRealtimeTranscriptionResult(data) {
            try {
                if (data.type === 'Results' && data.channel && data.channel.alternatives && data.channel.alternatives[0]) {
                    const transcript = data.channel.alternatives[0].transcript;
                    const isFinal = data.is_final;
                    
                    if (isFinal) {
                        if (transcript.trim()) {
                            stableRealtimeText += transcript + ' ';
                            partialRealtimeText = '';
                            hadTranscript = true;
                            lastTranscriptMs = Date.now();
                        }
                    } else {
                        partialRealtimeText = transcript;
                        if ((transcript || '').trim()) {
                            hadTranscript = true;
                            lastTranscriptMs = Date.now();
                        }
                    }
                    
                    updateTranscriptionDisplay();
                }
            } catch (error) {
                console.error('Error handling real-time transcription:', error);
            }
        }
        
        // Update transcription display
        function updateTranscriptionDisplay() {
            let displayHTML = '<strong>Live transcription:</strong> ';
            
            if (stableRealtimeText) {
                displayHTML += `<span style="color: #333;">${stableRealtimeText}</span>`;
            }
            
            if (partialRealtimeText) {
                displayHTML += `<span style="color: #007bff; background: #e7f3ff; padding: 2px 4px; border-radius: 3px; animation: pulse 1s infinite;">${partialRealtimeText}</span>`;
            }
            
            if (!stableRealtimeText && !partialRealtimeText) {
                displayHTML += '<span style="color: #999;">Listening for speech...</span>';
            }
            
            liveTranscriptEl.innerHTML = displayHTML;
        }
        
        // Stop recording
        function stopRecording() {
            if (!isRecording) return;
            isRecording = false;
            updateMicStatus('idle');
            
            if (deepgramWebSocket) {
                deepgramWebSocket.close();
                deepgramWebSocket = null;
            }
            
            if (realtimeProcessor) {
                realtimeProcessor.disconnect();
                realtimeProcessor = null;
            }
            
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close();
                audioContext = null;
            }
        }
        
        // Monitor inactivity and auto-finalize
        function tickInactive() {
            if (!isRecording || isFinalizing) return;
            
            const now = Date.now();
            const noSpeech = !hadTranscript;
            const compare = noSpeech ? recordingStartMs : lastTranscriptMs;
            const threshold = noSpeech ? INITIAL_SILENCE_MS : SPEECH_INACTIVITY_MS;
            
            if (now - compare >= threshold) {
                finalizeAnswer();
                return;
            }
            
            setTimeout(tickInactive, 500);
        }
        
        // Finalize answer and get next question
        async function finalizeAnswer() {
            if (isFinalizing) return;
            isFinalizing = true;
            
            stopRecording();
            
            const finalText = (stableRealtimeText + (partialRealtimeText ? (' ' + partialRealtimeText) : '')).trim();
            
            try {
                statusEl.innerText = 'Processing your answer...';
                
                const res = await fetch('/ai/upload_answer', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        session_id: sessionId,
                        transcript: finalText || ''
                    })
                });
                
                const data = await res.json();
                
                if (data.error) {
                    statusEl.innerText = 'Error: ' + data.error;
                    isFinalizing = false;
                    return;
                }
                
                // Handle completion
                if (data.completed) {
                    statusEl.innerText = data.message || 'Interview completed!';
                    finishBtn.classList.add('hidden');
                    
                    // Start coding round
                    setTimeout(() => {
                        startCodingRound();
                    }, 2000);
                    return;
                }
                
                // Next question
                currentQuestionNumber = data.question_number || (currentQuestionNumber + 1);
                updateProgress(currentQuestionNumber, maxQuestions);
                
                questionEl.innerText = data.next_question || '';
                statusEl.innerText = 'Playing next question...';
                
                if (data.audio_url) {
                    questionAudio.style.display = 'block';
                    questionAudio.src = data.audio_url;
                    questionAudio.onended = () => {
                        statusEl.innerText = 'Listening for your answer...';
                        isFinalizing = false;
                        beginRecording();
                    };
                    questionAudio.play().catch(() => {
                        isFinalizing = false;
                        beginRecording();
                    });
                } else {
                    isFinalizing = false;
                    setTimeout(beginRecording, 400);
                }
                
            } catch (error) {
                statusEl.innerText = 'Error processing answer: ' + error.message;
                isFinalizing = false;
            }
        }
        
        // Start coding round
        function startCodingRound() {
            statusEl.innerText = 'Starting coding challenge...';
            questionEl.innerHTML = `
                <h3>üíª Coding Challenge</h3>
                <p>Now you'll be given a coding problem to solve. You'll have access to a code editor and can run your code.</p>
                <p>Click the button below to start the coding challenge.</p>
                <button class="btn" onclick="window.location.href='?phase=coding'">Start Coding Challenge</button>
            `;
        }
        
        // Update microphone status
        function updateMicStatus(status) {
            if (status === 'recording') {
                micStatus.classList.remove('idle');
                micStatusText.textContent = 'Recording...';
            } else if (status === 'idle') {
                micStatus.classList.add('idle');
                micStatusText.textContent = 'Ready';
            } else if (status === 'preparing') {
                micStatus.classList.add('idle');
                micStatusText.textContent = 'Preparing...';
            }
        }
        
        // Update progress bar
        function updateProgress(current, total) {
            const percentage = (current / total) * 100;
            progressFill.style.width = percentage + '%';
            progressText.textContent = `Question ${current} of ${total}`;
        }
        
        // Setup proctoring
        async function setupProctoring() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                const video = document.getElementById('proctoringVideo');
                video.srcObject = stream;
                
                // Update session info
                const urlParams = new URLSearchParams(window.location.search);
                const sessionKey = urlParams.get('session_key');
                document.getElementById('sessionInfo').innerHTML = `
                    <strong>Session:</strong> ${sessionKey ? sessionKey.substring(0, 8) + '...' : 'Unknown'}<br>
                    <strong>Time:</strong> ${new Date().toLocaleTimeString()}
                `;
                
            } catch (error) {
                console.error('Proctoring setup failed:', error);
            }
        }
        
        // Finish button handler
        finishBtn.onclick = finalizeAnswer;
        
        // Initialize everything
        document.addEventListener('DOMContentLoaded', () => {
            setupProctoring();
            setTimeout(() => {
                startInterview();
            }, 1000);
        });
    </script>
</body>
</html>
