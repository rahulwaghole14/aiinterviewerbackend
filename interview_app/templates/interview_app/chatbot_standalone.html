<!DOCTYPE html>
<html><head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width,initial-scale=1'>
<title>AI Interview Bot</title>
<style>
body{font-family:'Segoe UI',Tahoma,Arial;margin:0;padding:16px;background:#f5f5f5;} 
.container{background:#fff;border-radius:10px;box-shadow:0 2px 10px rgba(0,0,0,.08);padding:20px;max-width:800px;margin:0 auto;}
.status{padding:15px;border-radius:8px;background:#e7f3ff;color:#0c5460;margin:15px 0;text-align:center;font-weight:600;}
.question{font-size:18px;font-weight:600;margin:20px 0;padding:15px;background:#f8f9fa;border-radius:8px;border-left:4px solid #007bff;white-space:pre-wrap;line-height:1.6;}
.status{white-space:pre-wrap;line-height:1.6;}
.bot-reply{background:#fff4e5;border-left:4px solid #ff9800;padding:12px 15px;border-radius:8px;margin:10px 0;display:none;white-space:pre-wrap;line-height:1.5;color:#8a4b00;font-weight:500;}
.transcript{background:#f8f9fa;padding:15px;border-radius:8px;margin:15px 0;border-left:3px solid #28a745;min-height:60px;white-space:pre-wrap;line-height:1.5;}
.progress{width:100%;height:20px;background:#e9ecef;border-radius:10px;overflow:hidden;margin:15px 0;}
.progress-fill{height:100%;background:#007bff;transition:width 0.3s ease;}
.mic-status{display:inline-block;width:20px;height:20px;border-radius:50%;margin-right:10px;background:#dc3545;animation:pulse 1s infinite;}
.mic-status.idle{background:#6c757d;animation:none;}
@keyframes pulse{0%{opacity:1;}50%{opacity:0.7;}100%{opacity:1;}}
audio{width:100%;margin:10px 0;}
</style>
</head>
<body>
<div class='container'>
  <h2 style='text-align:center;color:#333;margin-bottom:20px;'>üé§ AI Technical Interview</h2>
  <div class='progress'><div class='progress-fill' id='progressFill' style='width:0%'></div></div>
  <div style='text-align:center;margin-bottom:20px;'><span id='progressText'>Question 1 of 4</span></div>
  <div class='status' id='st'>Initializing interview...</div>
  <div id='botReply' class='bot-reply'></div>
  <div class='question' id='q'></div>
  <audio id='qa' controls style='display:none'></audio>
  <div style='text-align:center;margin:15px 0;'>
    <div class='mic-status' id='micStatus'></div>
    <span id='micStatusText'>Preparing...</span>
  </div>
  <div class='transcript' id='live'><strong>Live transcription:</strong> <span style='color:#999;'>Listening for speech...</span></div>
</div>

<script>
const SESSION_KEY="{{ session_key }}";
const DEEPGRAM_API_KEY='{{ DEEPGRAM_API_KEY|default:"" }}';

let sid=null,listening=false,upstreamReady=false,isFinalizing=false;
let ws=null,ctx=null,node=null,source=null,streamRef=null;
let stable='',partial='',hadTranscript=false;
let recordingStartMs=0,lastTranscriptMs=0;
const INITIAL_SILENCE_MS=4000,SPEECH_INACTIVITY_MS=5000;

function getProxyWsUrl(){
  const loc=window.location;
  const wsProto=loc.protocol==='https:'?'wss':'ws';
  return wsProto+'://'+loc.host+'/dg_ws';
}

async function api(url,body){
  const r=await fetch(url,{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify(body)});
  return await r.json();
}

async function start(){
  console.log('üöÄ Starting chatbot...');
  document.getElementById('st').innerText='Starting...';
  clearBotReply();
  const data=await api('/ai/start',{session_key:SESSION_KEY});
  console.log('‚úÖ Response from /ai/start:',data);
  console.log('üéµ Audio URL:', data.audio_url);
  console.log('‚ùì Question:', data.question);
  console.log('üìä Question number:', data.question_number);
  if(data.error){
    document.getElementById('st').innerText='Error: '+data.error;
    return;
  }
  sid=data.session_id;
  document.getElementById('q').innerText=data.question||'';
  document.getElementById('st').innerText='Question asked. Speak your answer...';
  document.getElementById('progressText').innerText='Question '+data.question_number+' of '+data.max_questions;
  document.getElementById('progressFill').style.width=((data.question_number/data.max_questions)*100)+'%';
  
  // Notify parent window that interview has started (for video recording)
  try{
    if(window.parent && window.parent !== window && window.parent.postMessage){
      window.parent.postMessage({type:'interview_started', session_key:SESSION_KEY},'*');
      console.log('üì§ Notified parent window: interview_started', {session_key:SESSION_KEY});
      // Also try direct call if parent is accessible
      if(window.parent.startVideoRecording && typeof window.parent.startVideoRecording === 'function'){
        console.log('üì§ Calling parent startVideoRecording directly...');
        window.parent.startVideoRecording();
      }
    }else{
      console.warn('‚ö†Ô∏è Cannot access parent window:', {
        hasParent: !!window.parent,
        isSameWindow: window.parent === window,
        hasPostMessage: !!(window.parent && window.parent.postMessage)
      });
    }
  }catch(e){
    console.warn('‚ö†Ô∏è Could not notify parent window:', e);
  }
  
  if(data.audio_url && data.audio_url.trim()){
    const qa=document.getElementById('qa');
    qa.style.display='block';
    qa.src=data.audio_url;
    qa.onended=()=>beginRecord();
    qa.play().catch(()=>beginRecord());
  }else{
    // Wait for Google Cloud TTS audio - if not available, wait a bit then start recording
    console.warn('‚ö†Ô∏è No audio_url provided - Google Cloud TTS may not be configured');
    setTimeout(beginRecord,600);
  }
}

async function beginRecord(){
  if(listening)return;
  listening=true;
  document.getElementById('st').innerText='üé§ Listening...';
  hadTranscript=false;
  stable='';
  partial='';
  
  // Reuse existing stream and context if available
  if(!streamRef || !streamRef.active){
    try{
      console.log('üé§ Requesting microphone access...');
      streamRef=await navigator.mediaDevices.getUserMedia({
        audio:{
          echoCancellation:false,
          noiseSuppression:false,
          autoGainControl:false,
          sampleRate:16000,
          channelCount:2
        }
      });
      console.log('‚úÖ Microphone access granted');
      console.log('üé§ Audio track settings:', streamRef.getAudioTracks()[0].getSettings());
    }catch(e){
      console.error('‚ùå Microphone error:', e);
      document.getElementById('st').innerText='Mic error: '+e.message;
      return;
    }
  }else{
    console.log('‚ôªÔ∏è Reusing existing microphone stream');
  }
  
  // Reuse existing audio context if available
  if(!ctx || ctx.state==='closed'){
    console.log('üéôÔ∏è Creating new audio context...');
    ctx=new (window.AudioContext||window.webkitAudioContext)();
    console.log('üéôÔ∏è Audio context native SR:',ctx.sampleRate);
    console.log('üéôÔ∏è Audio context state:',ctx.state);
  }else{
    console.log('‚ôªÔ∏è Reusing existing audio context, state:', ctx.state);
  }
  
  // Resume audio context if suspended
  if(ctx.state==='suspended'){
    await ctx.resume();
    console.log('‚úÖ Audio context resumed');
  }
  
  // CRITICAL: Only create source ONCE! Creating multiple sources from same stream breaks ScriptProcessor
  if(!source){
    source=ctx.createMediaStreamSource(streamRef);
    console.log('‚úÖ Media stream source created');
  }else{
    console.log('‚ôªÔ∏è Reusing existing media stream source');
  }
  
  // Use ScriptProcessor instead of AudioWorklet for better compatibility
  const USE_SCRIPT_PROCESSOR = true;
  
  async function setupWorklet(){
    console.log('üîß Setting up audio worklet...');
    const code=`class PCMWorkletProcessor extends AudioWorkletProcessor{
      process(inputs){
        // Handle both mono and stereo by reading all available channels
        if(inputs&&inputs[0]){
          const channels=inputs[0];
          if(channels.length===0)return true;
          
          // If stereo, mix down to mono
          if(channels.length===2){
            const left=channels[0];
            const right=channels[1];
            const mono=new Float32Array(left.length);
            for(let i=0;i<left.length;i++){
              mono[i]=(left[i]+right[i])/2;
            }
            this.port.postMessage(mono);
          }else{
            // Mono input
            this.port.postMessage(channels[0]);
          }
        }
        return true;
      }
    }
    registerProcessor('pcm-worklet',PCMWorkletProcessor);`;
    const blob=new Blob([code],{type:'application/javascript'});
    const url=URL.createObjectURL(blob);
    await ctx.audioWorklet.addModule(url);
    URL.revokeObjectURL(url);
    console.log('‚úÖ Audio worklet module loaded');
    
    node=new AudioWorkletNode(ctx,'pcm-worklet',{numberOfInputs:1,numberOfOutputs:1,outputChannelCount:[1]});
    console.log('‚úÖ Audio worklet node created');
    
    let packetCount = 0;
    node.port.onmessage=(ev)=>{
      packetCount++;
      if(packetCount % 50 === 1) {  // Log every 50th packet to reduce spam
        console.log('üéµ Audio worklet message received:', ev.data.length, 'samples, packet#', packetCount);
      }
      if(!ws||ws.readyState!==WebSocket.OPEN||!upstreamReady){
        if(packetCount % 50 === 1) {
          console.log('‚ö†Ô∏è WebSocket not ready - ws:', ws?.readyState, 'upstreamReady:', upstreamReady);
        }
        return;
      }
      const f32=ev.data;
      const i16=new Int16Array(f32.length);
      for(let i=0;i<f32.length;i++){
        let s=f32[i];
        s=Math.max(-1,Math.min(1,s));
        i16[i]=s<0?s*0x8000:s*0x7FFF;
      }
      // Calculate RMS to detect actual audio
      const rms = Math.sqrt(f32.reduce((sum, val) => sum + val * val, 0) / f32.length);
      if (rms > 0.001) { // Lower threshold to catch more audio
        console.log('üéµ Audio packet with sound detected!', { rms: rms.toFixed(4), samples: f32.length, bufferSize: i16.buffer.byteLength });
      }
      ws.send(i16.buffer);
    };
    const silent=ctx.createGain();
    silent.gain.value=0;
    source.connect(node);
    node.connect(silent).connect(ctx.destination);
    console.log('‚úÖ Audio worklet connected to audio graph');
  }
  
  function setupScriptProcessor(){
    console.log('üîß Setting up ScriptProcessor...');
    // Use buffer size 4096, match input channels with stream
    node=ctx.createScriptProcessor(4096,2,1);
    let packetCount = 0;
    let lastPacketTime = Date.now();
    node.onaudioprocess=(ev)=>{
      try{
        packetCount++;
        lastPacketTime = Date.now();
        if(packetCount % 50 === 1) {
          console.log('üéµ ScriptProcessor packet#', packetCount);
        }
        if(!ws||ws.readyState!==WebSocket.OPEN||!upstreamReady){
          if(packetCount % 50 === 1) {
            console.log('‚ö†Ô∏è WebSocket not ready');
          }
          return;
        }
        
        // Mix stereo to mono
        const left=ev.inputBuffer.getChannelData(0);
        const right=ev.inputBuffer.numberOfChannels > 1 ? ev.inputBuffer.getChannelData(1) : left;
        const mono=new Float32Array(left.length);
        for(let i=0;i<left.length;i++){
          mono[i]=(left[i]+right[i])/2;
        }
        
        // Calculate RMS
        const rms = Math.sqrt(mono.reduce((sum, val) => sum + val * val, 0) / mono.length);
        if (rms > 0.001) {
          console.log('üéµ Audio with sound detected!', { rms: rms.toFixed(4), samples: mono.length });
        }
        
        // Convert to INT16
        const i16=new Int16Array(mono.length);
        for(let i=0;i<mono.length;i++){
          let s=Math.max(-1,Math.min(1,mono[i]));
          i16[i]=s<0?s*0x8000:s*0x7FFF;
        }
        ws.send(i16.buffer);
      }catch(err){
        console.error('‚ùå ScriptProcessor error:', err);
      }
    };
    source.connect(node);
    node.connect(ctx.destination);
    console.log('‚úÖ ScriptProcessor connected to audio graph');
    
    // Keep a reference to prevent garbage collection
    window._activeAudioNode = node;
    window._activeAudioSource = source;
    window._activeAudioContext = ctx;
    
    // Monitor if callbacks stop
    const monitor = setInterval(() => {
      const now = Date.now();
      if (now - lastPacketTime > 500 && listening) {
        console.warn('‚ö†Ô∏è ScriptProcessor callbacks stopped! Last packet:', packetCount);
        console.log('üìä Audio context state:', ctx.state);
        console.log('üìä Node connected:', node.numberOfInputs, 'inputs,', node.numberOfOutputs, 'outputs');
        console.log('üìä Stream active:', streamRef.active, 'tracks:', streamRef.getTracks().map(t => t.readyState));
      }
    }, 1000);
    
    // Store monitor for cleanup
    window._audioMonitor = monitor;
  }
  
  // Always use ScriptProcessor for better audio capture
  if(USE_SCRIPT_PROCESSOR){
    console.log('üìä Using ScriptProcessor (forced for compatibility)');
    setupScriptProcessor();
  }else if(ctx.audioWorklet){
    try{
      await setupWorklet();
    }catch(_e){
      console.warn('AudioWorklet failed, using ScriptProcessor');
      setupScriptProcessor();
    }
  }else{
    setupScriptProcessor();
  }
  
  initWS(ctx.sampleRate||48000);
  recordingStartMs=Date.now();
  lastTranscriptMs=recordingStartMs;
  tickInactive();
}

function initWS(sampleRate){
  const url=getProxyWsUrl();
  console.log('üîê Connecting to Deepgram proxy...');
  ws=new WebSocket(url);
  ws.binaryType='arraybuffer';
  ws.onopen=()=>{
    console.log('‚úÖ Proxy connected, sending config for SR:',sampleRate);
    // Send config first to proxy
    ws.send(JSON.stringify({
      sample_rate:Number(sampleRate)||16000,
      model:'general'  // Changed from 'nova-2-meeting' - requires paid Deepgram tier
    }));
    upstreamReady=true;
  };
  ws.onmessage=(ev)=>{
    try{
      const data=JSON.parse(ev.data);
      console.log('üîä Deepgram response:', data); // Debug all responses
      if(data.type==='Results'&&data.channel&&data.channel.alternatives&&data.channel.alternatives[0]){
        const t=data.channel.alternatives[0].transcript||'';
        const isFinal=!!data.is_final;
        console.log('üìù Transcript:',[t],'final:',isFinal);
        if(isFinal){
          if(t.trim()){
            stable+=t+' ';
            hadTranscript=true;
            lastTranscriptMs=Date.now();
          }
          partial='';
        }else{
          partial=t;
          if(t.trim()){
            hadTranscript=true;
            lastTranscriptMs=Date.now();
          }
        }
        updateLive();
      }
    }catch(err){
      console.error('‚ùå Error parsing Deepgram message:', err, ev.data);
    }
  };
  ws.onclose=(ev)=>{
    console.log('‚ùå WebSocket closed:', ev.code, ev.reason);
    upstreamReady=false;
  };
  ws.onerror=(ev)=>{
    console.error('‚ùå WebSocket error:', ev);
  };
}

function stopRecord(){
  listening=false;
  console.log('‚èπÔ∏è Stopping record...');
  try{if(node){node.disconnect();node=null;}}catch(e){console.error('Error disconnecting node:', e);}
  // DON'T close audio context or stop stream - keep them for retry!
  // try{if(ctx&&ctx.state!=='closed'){ctx.close();}}catch(e){}
  // try{if(streamRef){streamRef.getTracks().forEach(t=>t.stop());}}catch(e){}
  try{if(ws){ws.close();}}catch(e){console.error('Error closing WebSocket:', e);}
  upstreamReady=false;
  console.log('‚úÖ Record stopped (keeping audio context and stream alive)');
}

function updateLive(){
  const combined=(stable+(partial?(' '+partial):'')).trim();
  document.getElementById('live').innerHTML='<strong>Live transcription:</strong> '+(combined||'<span style="color:#999">Listening...</span>');
}

function tickInactive(){
  if(!listening||isFinalizing)return;
  const now=Date.now();
  const noSpeech=!hadTranscript;
  const compare=noSpeech?recordingStartMs:lastTranscriptMs;
  const th=noSpeech?INITIAL_SILENCE_MS:SPEECH_INACTIVITY_MS;
  if(now-compare>=th){finalize();return;}
  setTimeout(tickInactive,500);
}

async function finalize(){
  if(isFinalizing)return;
  isFinalizing=true;
  stopRecord();
  const finalText=(stable+(partial?(' '+partial):'')).trim();
  console.log('üé¨ Finalizing:',finalText);
  try{
    const data=await api('/ai/upload_answer',{
      session_id:sid,
      transcript:finalText||'',
      silence:finalText?0:1,
      had_voice:hadTranscript?1:0
    });
    console.log('üì• Response:',data);
    if(data.error){
      document.getElementById('st').innerText='Error: '+data.error;
      isFinalizing=false;
      return;
    }
    if(data.completed){
      document.getElementById('st').innerText=data.message||'Completed';
      clearBotReply();
      const finishInterview=()=>{
      if(window.parent&&window.parent.postMessage){
        window.parent.postMessage({type:'qa_completed'},'*');
        }
      };
      if(data.final_audio_url){
        const qa=document.getElementById('qa');
        qa.style.display='block';
        qa.src=data.final_audio_url;
        qa.onended=finishInterview;
        qa.play().catch(finishInterview);
      }else{
        finishInterview();
      }
      return;
    }

    if(data.follow_up_answer){
      document.getElementById('st').innerText='Answering your question...';
      showBotReply(data.follow_up_answer);
      if(data.repeat_question){
        document.getElementById('q').innerText=data.repeat_question;
      }
      const resumeRecording=()=>{
        isFinalizing=false;
        setTimeout(beginRecord,800);
      };
      if(data.follow_up_audio_url){
        const qa=document.getElementById('qa');
        qa.style.display='block';
        qa.src=data.follow_up_audio_url;
        qa.onended=resumeRecording;
        qa.play().catch(resumeRecording);
      }else{
        resumeRecording();
      }
      return;
    }
    if(data.acknowledge){
      document.getElementById('st').innerText=data.message||"I didn't catch that.";
      stable='';
      partial='';
      hadTranscript=false;
      updateLive();
      isFinalizing=false;
      setTimeout(beginRecord,800);
      return;
    }
    // Next question
    clearBotReply();
    document.getElementById('q').innerText=data.next_question||'';
    document.getElementById('st').innerText='Question asked. Speak your answer...';
    document.getElementById('progressText').innerText='Question '+data.question_number+' of '+data.max_questions;
    document.getElementById('progressFill').style.width=((data.question_number/data.max_questions)*100)+'%';
    stable='';
    partial='';
    hadTranscript=false;
    updateLive();
    
    if(data.audio_url && data.audio_url.trim()){
      const qa=document.getElementById('qa');
      qa.style.display='block';
      qa.src=data.audio_url;
      qa.onended=()=>{isFinalizing=false;beginRecord();};
      try{
        await qa.play();
      }catch(_e){
        isFinalizing=false;
        beginRecord();
      }
    }else{
      // Wait for Google Cloud TTS audio - if not available, wait a bit then start recording
      console.warn('‚ö†Ô∏è No audio_url provided - Google Cloud TTS may not be configured');
      isFinalizing=false;
      setTimeout(beginRecord,600);
    }
  }catch(e){
    console.error('Finalize error:',e);
    document.getElementById('st').innerText='Error: '+e.message;
    isFinalizing=false;
  }
}

// Auto-start
setTimeout(()=>{
  console.log('üöÄ Auto-starting interview...');
  start();
},500);

function showBotReply(text){
  const el=document.getElementById('botReply');
  if(!el)return;
  el.style.display='block';
  el.textContent=text;
}

function clearBotReply(){
  const el=document.getElementById('botReply');
  if(!el)return;
  el.style.display='none';
  el.textContent='';
}
</script>
</body>
</html>
