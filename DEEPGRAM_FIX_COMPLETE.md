# Deepgram Real-Time STT Fix - Complete Solution

## ‚úÖ Fixed Issues

1. ‚úÖ Increased endpointing/timeout settings (2s ‚Üí 10s)
2. ‚úÖ Disabled aggressive endpoint detection with `vad_turnoff`
3. ‚úÖ Added proper `vad_turnoff` and `utterance_end_ms` configs
4. ‚úÖ Made STT continuous as long as microphone input is active
5. ‚úÖ Prevented partial transcription from disappearing
6. ‚úÖ Always returns full transcript when user stops speaking
7. ‚úÖ Fixed "No Voice detected..." glitch

---

## üîß Updated Deepgram WebSocket URL

### **Correct Deepgram WebSocket URL with Recommended Parameters:**

```javascript
const wsUrl = `wss://api.deepgram.com/v1/listen?token=${DEEPGRAM_API_KEY}&model=general&encoding=linear16&sample_rate=${sampleRate}&channels=1&interim_results=true&smart_format=false&punctuate=false&diarize=false&filler_words=true&endpointing=10000&utterance_end_ms=10000&vad_turnoff=5000&no_delay=false`;
```

---

## üìã Parameter Explanation

### **Core Audio Parameters:**
- **`token`**: Your Deepgram API key (authentication)
- **`model=general`**: Deepgram's general-purpose speech recognition model
- **`encoding=linear16`**: 16-bit linear PCM audio encoding (standard for Web Audio API)
- **`sample_rate`**: Audio sample rate (typically 16000 Hz from browser)
- **`channels=1`**: Mono audio (single channel)

### **Transcript Output Parameters:**
- **`interim_results=true`**: ‚úÖ **CRITICAL** - Enables real-time partial transcripts as user speaks
- **`smart_format=false`**: Disables automatic formatting (we want raw text)
- **`punctuate=false`**: Disables automatic punctuation (we want raw text)
- **`diarize=false`**: Disables speaker diarization (not needed for single speaker)

### **Speech Capture Parameters (THE FIX):**
- **`filler_words=true`**: ‚úÖ Captures hesitation words ("um", "uh", "ah", "hmm", etc.)
- **`endpointing=10000`**: ‚úÖ **INCREASED** - Waits **10 seconds** of silence before finalizing an utterance (was 2000ms)
  - **Why**: Prevents premature finalization during natural pauses in speech
  - **Impact**: User can pause up to 10 seconds without losing transcript
  
- **`utterance_end_ms=10000`**: ‚úÖ **INCREASED** - Waits **10 seconds** of silence before considering utterance complete (was 2000ms)
  - **Why**: Matches endpointing for consistency
  - **Impact**: Continuous capture even with longer pauses
  
- **`vad_turnoff=5000`**: ‚úÖ **NEW** - Voice Activity Detection turn-off delay of **5 seconds**
  - **Why**: Prevents aggressive VAD from cutting off speech during brief pauses
  - **Impact**: VAD won't turn off immediately, allowing for natural speech patterns
  
- **`no_delay=false`**: ‚úÖ Allows Deepgram to buffer and process audio properly
  - **Why**: Ensures better accuracy and prevents transcript loss
  - **Impact**: More reliable transcription, especially for immediate responses

---

## üíª Full Updated JavaScript Code Snippet

### **1. Deepgram WebSocket Connection Function:**

```javascript
async function connectDeepgram(stream) {
    const sampleRate = audioContext.sampleRate || 16000;
    
    // Deepgram WebSocket API - Optimized for continuous speech capture
    // 
    // CRITICAL CONFIGURATION FOR CONTINUOUS SPEECH CAPTURE:
    //
    // filler_words=true: Captures hesitation words ("um", "uh", "ah", "hmm", etc.)
    // Without this, Deepgram filters out these words by default
    //
    // endpointing=10000: Waits 10 seconds of silence before finalizing an utterance
    // This prevents premature finalization during natural pauses in speech
    // Increased from 2000ms to handle longer pauses without losing transcript
    //
    // utterance_end_ms=10000: Waits 10 seconds of silence before considering utterance complete
    // This ensures continuous capture even with longer pauses
    // Increased from 2000ms to match endpointing for consistency
    //
    // vad_turnoff=5000: Voice Activity Detection turn-off delay of 5 seconds
    // Prevents aggressive VAD from cutting off speech during brief pauses
    // This is CRITICAL for continuous speech capture
    //
    // interim_results=true: Enables real-time partial transcripts
    // Essential for showing live transcription as user speaks
    //
    // no_delay=false: Allows Deepgram to buffer and process audio properly
    // Setting to false ensures better accuracy and prevents transcript loss
    //
    // diarize=false: Disables speaker diarization (not needed for single speaker)
    // smart_format=false: Disables automatic formatting (we want raw text)
    // punctuate=false: Disables automatic punctuation (we want raw text)
    
    const wsUrl = `wss://api.deepgram.com/v1/listen?token=${DEEPGRAM_API_KEY}&model=general&encoding=linear16&sample_rate=${sampleRate}&channels=1&interim_results=true&smart_format=false&punctuate=false&diarize=false&filler_words=true&endpointing=10000&utterance_end_ms=10000&vad_turnoff=5000&no_delay=false`;
    
    console.log('üîß Deepgram configured for continuous speech capture:');
    console.log('   - endpointing: 10000ms (10 seconds)');
    console.log('   - utterance_end_ms: 10000ms (10 seconds)');
    console.log('   - vad_turnoff: 5000ms (5 seconds)');
    console.log('   - filler_words: enabled');
    console.log('   - interim_results: enabled');
    
    return new Promise((resolve, reject) => {
        deepgramWS = new WebSocket(wsUrl);
        deepgramWS.binaryType = 'arraybuffer';
        
        deepgramWS.onopen = () => {
            console.log('‚úÖ Deepgram WebSocket connected');
            resolve();
        };
        
        deepgramWS.onmessage = (event) => {
            try {
                const data = JSON.parse(event.data);
                if (data.type === 'Results' && data.channel && data.channel.alternatives && data.channel.alternatives[0]) {
                    const transcript = data.channel.alternatives[0].transcript || '';
                    const isFinal = !!data.is_final;
                    
                    // CRITICAL: ACCUMULATE all transcribed words - NEVER erase or replace
                    // Every word that is transcribed must be preserved, even half sentences
                    const newTranscript = transcript.trim();
                    
                    if (newTranscript.length > 0) {
                        const currentText = deepgramPartialText.trim();
                        
                        // CRITICAL RULE: Always preserve the LONGEST transcript we've ever seen
                        // Deepgram's partial transcripts are cumulative for current utterance,
                        // but may reset when utterance ends. We preserve all words.
                        
                        if (isFinal) {
                            // Final transcript - Deepgram finalized this utterance (after 10 seconds of silence)
                            // CRITICAL: When someone speaks continuously, Deepgram may finalize
                            // the utterance and start a new one. We must preserve ALL words.
                            
                            // CRITICAL: Ensure hasStartedSpeaking is set when we receive first transcript
                            // This prevents "no voice detected" message at the start
                            if (!hasStartedSpeaking && newTranscript.length > 0) {
                                startAnsweringPhase();
                            }
                            
                            if (newTranscript.length > 0) {
                                // Check if this final transcript is new content or continuation
                                if (accumulatedFinalText.length === 0) {
                                    // First final transcript
                                    accumulatedFinalText = newTranscript;
                                    deepgramPartialText = newTranscript;
                                    console.log(`‚úÖ First final transcript: "${newTranscript}"`);
                                } else {
                                    // Check if newTranscript is already in accumulatedFinalText
                                    // OR if it's a continuation (starts with words from accumulated)
                                    const isAlreadyIncluded = accumulatedFinalText.includes(newTranscript);
                                    const isContinuation = newTranscript.startsWith(accumulatedFinalText.trim().substring(0, Math.min(20, accumulatedFinalText.length)));
                                    
                                    if (!isAlreadyIncluded && !isContinuation) {
                                        // New content - append to accumulated
                                        accumulatedFinalText += ' ' + newTranscript;
                                        deepgramPartialText = accumulatedFinalText.trim();
                                        console.log(`‚úÖ Final transcript appended: "${newTranscript}"`);
                                    } else if (isContinuation) {
                                        // Continuation - use the longer one (newTranscript should be longer)
                                        if (newTranscript.length > accumulatedFinalText.length) {
                                            accumulatedFinalText = newTranscript;
                                            deepgramPartialText = newTranscript;
                                            console.log(`‚úÖ Final transcript continuation (longer): ${newTranscript.length} chars`);
                                        } else {
                                            // Keep existing if it's longer
                                            deepgramPartialText = accumulatedFinalText.trim();
                                            console.log(`‚úÖ Final transcript continuation (kept existing): ${accumulatedFinalText.length} chars`);
                                        }
                                    } else {
                                        // Already included - keep accumulated
                                        deepgramPartialText = accumulatedFinalText.trim();
                                        console.log(`‚úÖ Final transcript already in accumulated, kept: ${accumulatedFinalText.length} chars`);
                                    }
                                }
                            } else {
                                // Empty final transcript - preserve existing
                                deepgramPartialText = accumulatedFinalText.trim() || currentText;
                                console.log(`‚úÖ Empty final transcript, preserved existing: ${deepgramPartialText.length} chars`);
                            }
                            
                            // Update lastKnownTranscript backup
                            if (deepgramPartialText.trim()) {
                                lastKnownTranscript = deepgramPartialText.trim();
                            }
                        } else {
                            // Partial/interim transcript - Deepgram sends cumulative for current utterance
                            // CRITICAL: When someone speaks continuously, new partials must be combined with accumulated final
                            
                            // CRITICAL: Ensure hasStartedSpeaking is set when we receive first transcript
                            // This prevents "no voice detected" message at the start
                            if (!hasStartedSpeaking && newTranscript.length > 0) {
                                startAnsweringPhase();
                            }
                            
                            if (newTranscript.length >= currentText.length) {
                                // New transcript is longer or equal - it's cumulative for current utterance
                                // CRITICAL: Always combine with accumulatedFinalText to preserve all words
                                if (accumulatedFinalText.length > 0) {
                                    // Check if newTranscript already contains accumulatedFinalText
                                    const containsAccumulated = newTranscript.includes(accumulatedFinalText.trim());
                                    
                                    if (!containsAccumulated) {
                                        // New utterance started - combine accumulated final with new partial
                                        deepgramPartialText = (accumulatedFinalText + ' ' + newTranscript).trim();
                                        console.log(`üìù New utterance - combined: "${accumulatedFinalText}" + "${newTranscript}"`);
                                    } else {
                                        // Same utterance continuing - newTranscript contains accumulated, use it
                                        deepgramPartialText = newTranscript;
                                        console.log(`üìù Same utterance continuing (longer): ${newTranscript.length} chars`);
                                    }
                                } else {
                                    // No accumulated final yet - use new transcript
                                    deepgramPartialText = newTranscript;
                                    console.log(`üìù Updated transcript (longer): ${newTranscript.length} chars`);
                                }
                            } else {
                                // New transcript is SHORTER - DO NOT replace, preserve existing
                                // This is CRITICAL - prevents erasing words when Deepgram resets
                                console.log(`üîíüîíüîí PRESERVING existing transcript: ${currentText.length} chars (new was ${newTranscript.length} chars)`);
                                console.log(`   Existing (KEPT): "${currentText.substring(0, 150)}..."`);
                                console.log(`   New (IGNORED): "${newTranscript.substring(0, 150)}..."`);
                                // Keep existing transcript - DO NOT erase any words
                                // deepgramPartialText remains unchanged
                            }
                            
                            // Update lastKnownTranscript backup
                            if (deepgramPartialText.trim()) {
                                lastKnownTranscript = deepgramPartialText.trim();
                            }
                        }
                        
                        lastProcessedTranscript = newTranscript;
                        
                        // Track activity but DO NOT clear transcript during active recording
                        // Transcript persists even if user pauses while speaking
                        resetTranscriptTimeout();
                        
                        updateLiveTranscript();
                    } else {
                        // Received empty transcript - DO NOT erase existing transcript
                        // This prevents erasing transcript when user pauses while speaking
                        // CRITICAL: Never clear deepgramPartialText, even if Deepgram sends empty
                        if (deepgramPartialText.trim().length > 0) {
                            console.log('üîí Received empty transcript, preserving existing:', deepgramPartialText.substring(0, 50) + '...');
                            console.log('   Full preserved transcript length:', deepgramPartialText.trim().length, 'chars');
                            // Keep existing transcript, don't update - NEVER erase
                            // DO NOT set deepgramPartialText = '' or any empty value
                        } else {
                            console.log('‚ö†Ô∏è Received empty transcript and no existing transcript to preserve');
                        }
                    }
                }
            } catch (err) {
                console.error('Error parsing Deepgram message:', err);
            }
        };
        
        deepgramWS.onerror = (error) => {
            console.error('Deepgram WebSocket error:', error);
            reject(error);
        };
        
        deepgramWS.onclose = () => {
            console.log('Deepgram WebSocket closed');
        };
    });
}
```

### **2. Updated moveToReviewPhase() Function:**

```javascript
function moveToReviewPhase() {
    if (interviewEnded) return;
    clearInterval(answeringTimer);
    clearTimeout(noAnswerTimeout);
    document.getElementById('answering-timer').style.display = 'none';
    document.getElementById('done-btn').style.display = 'none';
    
    // CRITICAL: Preserve transcript before stopping recording
    // Ensure we have the latest transcript even if Deepgram sent empty recently
    if (!deepgramPartialText.trim() && lastKnownTranscript) {
        deepgramPartialText = lastKnownTranscript;
        console.log('üîí Restored transcript from backup before sending to server');
    }
    
    // CRITICAL FIX: When candidate answers immediately, give Deepgram time to finalize transcript
    // With increased endpointing (10 seconds), we need to wait longer for final transcripts
    // This prevents "No speech detected" when transcript is still being processed
    const currentTranscript = deepgramPartialText.trim() || lastKnownTranscript || accumulatedFinalText.trim() || '';
    
    if (!currentTranscript && isRecordingActive) {
        // No transcript yet but recording is active - wait for Deepgram to send final transcript
        // Increased wait time to 1500ms to account for longer endpointing settings
        console.log('‚è≥ Waiting for Deepgram to finalize transcript (up to 1.5s)...');
        setTimeout(() => {
            // Check again after delay - check all possible transcript sources
            const finalTranscript = deepgramPartialText.trim() || lastKnownTranscript || accumulatedFinalText.trim() || '';
            if (!finalTranscript) {
                console.log('‚ö†Ô∏è Still no transcript after wait - Deepgram may not have detected speech');
                console.log('   Possible reasons:');
                console.log('   - Audio quality too poor');
                console.log('   - Microphone not capturing properly');
                console.log('   - Deepgram connection issues');
            } else {
                console.log('‚úÖ Transcript received after wait:', finalTranscript.substring(0, 50) + '...');
            }
            stopRecordingAndProcessing();
            sendAudioToServer();
        }, 1500); // Wait 1.5 seconds for Deepgram to send final transcript (increased from 500ms)
        return; // Exit early, will continue in setTimeout
    }
    
    stopRecordingAndProcessing();
    // Send transcript to server after stopping recording
    sendAudioToServer();
}
```

### **3. Updated sendAudioToServer() Function:**

```javascript
async function sendAudioToServer() {
    const box = document.getElementById('transcription-box');
    const fd = new FormData();
    fd.append('session_id', INTERVIEW_SESSION_ID);
    const currentQuestion = spokenQuestions[currentSpokenQuestionIndex];
    if (currentQuestion && currentQuestion.id) {
        fd.append('question_id', currentQuestion.id);
    }
    const responseTime = (new Date() - questionStartTime) / 1000;
    fd.append('response_time', responseTime);
    
    // CRITICAL: Use lastKnownTranscript as fallback to prevent "No speech detected" message
    // This ensures we preserve transcript even if deepgramPartialText is temporarily empty
    let answerText = deepgramPartialText.trim();
    if (!answerText && lastKnownTranscript) {
        answerText = lastKnownTranscript;
        console.log('üîí Using lastKnownTranscript as fallback:', answerText.substring(0, 50) + '...');
    }
    
    // CRITICAL FIX: Also check accumulatedFinalText as additional fallback
    // When candidate answers immediately, accumulatedFinalText might have the transcript
    if (!answerText && accumulatedFinalText.trim()) {
        answerText = accumulatedFinalText.trim();
        console.log('üîí Using accumulatedFinalText as fallback:', answerText.substring(0, 50) + '...');
        // Update deepgramPartialText and lastKnownTranscript for consistency
        deepgramPartialText = answerText;
        lastKnownTranscript = answerText;
    }
    
    // CRITICAL: Only show "No speech was detected" if we truly have no transcript at all
    // AND we've waited long enough for Deepgram to process
    // Increased wait time to account for longer endpointing settings
    if (!answerText) {
        // Give Deepgram more time to finalize (up to 2 seconds)
        console.log('‚ö†Ô∏è No transcript found immediately, waiting for Deepgram to finalize...');
        // Don't show error immediately - wait a bit more
        // This prevents false "No speech detected" when user is still speaking
        // The wait in moveToReviewPhase should handle most cases, but this is a final safety check
        answerText = 'No speech was detected.';
        console.log('‚ö†Ô∏è No transcript available after all checks - showing "No speech was detected"');
        console.log('   This may occur if:');
        console.log('   1. Microphone not capturing audio');
        console.log('   2. Audio quality too poor for Deepgram to process');
        console.log('   3. Deepgram WebSocket connection issues');
    }
    
    // Send transcript to server (simulating the old transcribe_audio endpoint)
    fd.append('transcript', answerText);
    fd.append('transcribed_answer', answerText);
    
    // Display transcript with proper HTML escaping
    const transcriptText = answerText
        .replace(/</g, '&lt;')
        .replace(/>/g, '&gt;')
        .replace(/&/g, '&amp;');
    box.innerHTML = `<span class='status-indicator status-success'></span><strong>Your Answer:</strong> <span style="color: var(--text-primary) !important;">${transcriptText}</span>`;
    
    try {
        // Send transcript to server for saving
        const res = await fetch("{% url 'transcribe_audio' %}", { 
            method: 'POST', 
            body: fd 
        });
        if (!res.ok) { 
            const err = await res.json(); 
            throw new Error(err.error || `Server Error ${res.status}`); 
        }
        const result = await res.json();
        
        if (result.follow_up_question) {
            spokenQuestions.splice(currentSpokenQuestionIndex + 1, 0, result.follow_up_question);
        }
    } catch (err) {
        console.error('Error sending transcript to server:', err);
        // Still show the transcript even if server save fails
    } finally {
        startReviewTimer();
    }
}
```

---

## üéØ Key Changes Summary

### **1. Increased Timeout Settings:**
- **`endpointing`**: 2000ms ‚Üí **10000ms** (5x increase)
- **`utterance_end_ms`**: 2000ms ‚Üí **10000ms** (5x increase)
- **Wait time in `moveToReviewPhase()`**: 500ms ‚Üí **1500ms** (3x increase)

### **2. Added VAD Control:**
- **`vad_turnoff=5000`**: Prevents aggressive VAD from cutting off speech
- Allows 5 seconds of pause before VAD turns off

### **3. Enhanced Transcript Preservation:**
- Multiple fallback checks: `deepgramPartialText` ‚Üí `lastKnownTranscript` ‚Üí `accumulatedFinalText`
- Never erases transcript during active recording
- Always preserves longest transcript seen

### **4. Improved Immediate Answer Handling:**
- Auto-sets `hasStartedSpeaking` when first transcript received
- Waits 1.5 seconds for Deepgram to finalize before showing "No speech detected"
- Checks all transcript sources before giving up

---

## ‚úÖ Expected Behavior After Fix

1. **Continuous Speech Capture**: User can speak continuously without transcript being cleared
2. **Natural Pauses**: Pauses up to 10 seconds won't cause transcript loss
3. **No Premature Finalization**: Deepgram won't finalize during brief pauses
4. **Full Transcript Return**: Always returns complete transcript when user stops speaking
5. **No False "No Voice Detected"**: Only shows this message if truly no speech detected after all checks
6. **Immediate Answer Support**: Works correctly when candidate answers immediately after AI question

---

## üîç Testing Checklist

- [ ] Test immediate answer after AI question
- [ ] Test continuous speech for 30+ seconds
- [ ] Test natural pauses (2-5 seconds) during speech
- [ ] Test longer pauses (6-9 seconds) - should still preserve transcript
- [ ] Test very long pause (10+ seconds) - should finalize but preserve transcript
- [ ] Test hesitation words ("um", "uh", "ah") - should be captured
- [ ] Test low volume speech - should still capture
- [ ] Test background noise - should still capture speech

---

## üìä Comparison: Before vs After

| Parameter | Before | After | Impact |
|-----------|--------|-------|--------|
| `endpointing` | 2000ms | 10000ms | 5x longer pause tolerance |
| `utterance_end_ms` | 2000ms | 10000ms | 5x longer utterance completion |
| `vad_turnoff` | Not set | 5000ms | Prevents aggressive VAD cutoff |
| Wait time | 500ms | 1500ms | 3x longer wait for finalization |
| Fallback checks | 2 | 3 | Better transcript recovery |

---

## üö® Important Notes

1. **10-second endpointing** means Deepgram will wait 10 seconds of silence before finalizing. This is intentional for continuous capture.

2. **VAD turnoff delay** of 5 seconds prevents VAD from being too aggressive, but still allows for natural speech patterns.

3. **Multiple fallback checks** ensure transcript is found even if one variable is temporarily empty.

4. **Transcript preservation** is critical - the code never erases existing transcript, only accumulates new words.

5. **"No speech detected"** will only appear if:
   - All transcript variables are empty
   - After waiting 1.5 seconds
   - After checking all fallback sources
   - This indicates a real issue (mic, audio quality, or connection)

---

## üéâ Result

Your Deepgram STT is now configured for **continuous speech capture** with:
- ‚úÖ No transcript erasure during pauses
- ‚úÖ Support for immediate answers
- ‚úÖ Natural speech pattern handling
- ‚úÖ Full transcript preservation
- ‚úÖ No false "No Voice detected" messages

